{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddefcc9b",
   "metadata": {},
   "source": [
    "# 3. Model Building\n",
    "\n",
    "##  Creating and Tuning ML Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55214024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jamie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import joblib \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a56dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1985, (27513, 7), (18868, 7))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data and tasting_note_list\n",
    "df = pd.read_csv('./data/branded.csv', index_col = 'Unnamed: 0')\n",
    "df_sms = pd.read_csv('./data/branded_singlemalts.csv', index_col = 'Unnamed: 0')\n",
    "\n",
    "with open('./data/tasting_notes.txt', 'r') as f:\n",
    "    wordlist = [item[:-1] for item in f.readlines()]\n",
    "\n",
    "len(wordlist), df.shape, df_sms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4834a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe df had 27513 rows. After dropping rows with 10 or fewer reviews, it now has 26953 rows.\n",
      "Dataframe df_sms had 18868 rows. After dropping rows with 10 or fewer reviews, it now has 18780 rows.\n"
     ]
    }
   ],
   "source": [
    "# dropping brands that only have a frequency of lower_limit or fewer because it screws\n",
    "# up cross-validation.\n",
    "def drop_unpopular_brands(name, df, lower_limit=0):\n",
    "    print(\"Dataframe {} had {} rows. After dropping rows with {} or fewer reviews, \".format(\n",
    "        name, df.shape[0], lower_limit), end=\"\")\n",
    "    dfbrandcounts = df['brand'].value_counts()\n",
    "    limited_list = dfbrandcounts[dfbrandcounts>lower_limit].index\n",
    "    df = df[df['brand'].isin(limited_list)]\n",
    "    print(\"it now has {} rows.\".format(df.shape[0]))\n",
    "    return df\n",
    "\n",
    "lower_limit = 10\n",
    "df = drop_unpopular_brands('df', df, 10)\n",
    "df_sms = drop_unpopular_brands('df_sms', df_sms, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3203d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_and_stem_text(text):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    text (string) - what you want to be lemmatised\n",
    "    OUTPUTS:\n",
    "    lemmas (list) - list of lemmatised next\n",
    "    '''\n",
    "    # Import stopword list and update with a few of my own\n",
    "    stopword_list = stopwords.words(\"english\")\n",
    "    [stopword_list.append(i) for i in ['nose', 'palate', 'taste', 'finish']]\n",
    "    \n",
    "    # Normalise text - remove numbers too as we don't need them\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenise\n",
    "    words = text.split()\n",
    "    \n",
    "    # Checks it's a word and removes stop words\n",
    "    words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Create stemmer object\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Add lemmas\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemmas.append(stemmer.stem(word))\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f4896",
   "metadata": {},
   "source": [
    "---\n",
    "### Initial attempt\n",
    "\n",
    "What follows here was my initial attempt to create a tokeniser and classifier of the whiskies. For some reason, I got it into my head that I needed to use the custom vocabulary list and apply a vectoriser for nose, palate and finish, and then start reshaping the sparse matrices that came out of it by deleting empty columns. I later realised that this indicates a misunderstanding of what a sparse matrix is and how it is handled, and thus this was all a waste of time that I couldn't get to work inside a pipeline anyway, but I've included it here as it's somewhat interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1d465bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectoriser_with_wordlist = CountVectorizer(tokenizer=tokenise_and_stem_text, vocabulary=wordlist)\n",
    "\n",
    "#X_nose = vectoriser_with_wordlist.fit_transform(df['nose'])\n",
    "#X_palate = vectoriser_with_wordlist.fit_transform(df['palate'])\n",
    "#X_finish = vectoriser_with_wordlist.fit_transform(df['finish'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5a4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(50):\n",
    "#    print(\"There are {} columns in the Nose matrix with {} '1's in it.\".format(\n",
    "#    (X_nose[X_nose.sum(axis=0)==i]).shape[1], i))\n",
    "#    print(\"There are {} columns in the Palate matrix with {} '1's in it.\".format(\n",
    "#    (X_palate[X_palate.sum(axis=0)==i]).shape[1], i))\n",
    "#   print(\"There are {} columns in the Finish matrix with {} '1's in it.\".format(\n",
    "#   (X_finish[X_finish.sum(axis=0)==i]).shape[1], i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443daf4b",
   "metadata": {},
   "source": [
    "Here I realised that you can dramatically cut down the size of the matrix by dropping any columns that have fewer than 4 hits in them. But even using a lower bound of 2 would substantially reduce their size. \n",
    "\n",
    "Again, this shows something of a misconception. Unique hits like this tend to be informationally _more_ valuable than the opposite. After all, the point is to discriminate, and something that uniquely picks out one whisky is fairly useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "321cc3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smaller_matrices(matrix, lowerbound=0, wordlist=wordlist):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    matrix - matrix you wish to make smaller \n",
    "    lowerbound - inclusive. The smallest total number of hits in the column of the matrix that\n",
    "                you'll allow. e.g. if there's only 1 review that mentions 'ablaze',\n",
    "                the sum for axis=0 will be 1. Worth keeping? Probably not...\n",
    "    wordlist - list of words that are indexed in the same way as matrix\n",
    "    OUTPUTS:\n",
    "    smaller_matrix - smaller matrix to use \n",
    "    smaller_list - smaller list of words.\n",
    "    '''\n",
    "    # Get indices of the columns we want to keep (i.e. where col sums >= lowerbound)\n",
    "    row_index, col_index = np.where(matrix.sum(axis=0)>=lowerbound)\n",
    "        \n",
    "    smaller_list = []\n",
    "    for i in range(len(col_index)):\n",
    "        column_to_add = matrix.toarray()[:,col_index[i]].copy().reshape(-1, 1)\n",
    "        if i == 0:\n",
    "            # Create new_matrix with first column\n",
    "            smaller_matrix = column_to_add\n",
    "        else:\n",
    "            # Concatenate new_matrix with i'th column. \n",
    "            smaller_matrix = np.hstack((smaller_matrix, column_to_add))\n",
    "           \n",
    "        # Add to newlist in order they appear in wordlist\n",
    "        smaller_list.append(wordlist[col_index[i]])\n",
    "        \n",
    "    return smaller_matrix, smaller_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6789572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller matrices for each in turn with a lowerbound of 3. \n",
    "#lb = 3\n",
    "#nose_matrix, nose_list = create_smaller_matrices(X_nose, lowerbound=lb, wordlist=wordlist)\n",
    "#palate_matrix, palate_list = create_smaller_matrices(X_palate, lowerbound=lb, wordlist=wordlist)\n",
    "#finish_matrix, finish_list = create_smaller_matrices(X_finish, lowerbound=lb, wordlist=wordlist)\n",
    "\n",
    "# Overlapping areas in terms of tasting notes between the three domains:\n",
    "#overlap_list = sorted((set(nose_list) & set(palate_list)) & set(finish_list))\n",
    "#nose_matrix.shape[1], palate_matrix.shape[1], finish_matrix.shape[1], len(overlap_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f45190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nose_list = ['nose_'+item for item in nose_list]\n",
    "#nose_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144cc8d7",
   "metadata": {},
   "source": [
    "My comment at the time was:\n",
    "\n",
    ">Now I'm finally able to produce a matrix of tasting notes, separated by nose, palate and finish sections for each row of the df_collated dataframe. The overlap list might be helpful down the line if I want to perform analyses along the lines of 'Which whiskies are the most different from nose to palate?' or similar. The next tasks I need to perform are to hstack these matrices together to create a matrix of shape 27513 x 3052 (or whatever it is, depending on the hyperparameters used), and keep track of what each column means. Then we need to start training a Classifier using the matrix. \n",
    "\n",
    ">At this point, I need to write this as a proper function and put it inside a pipeline, because I'll need to split the data into a training and testing set in order to evaluate different potential classifiers. Technically, I already have a bit of data leakage here because I constructed my vocublary list using the full dataset. However, it's not a serious leakage as I was mainly just including as many tasting-y sounding words as possible in that wordlist. Since very rarely used words aren't going to be that important anyway, it's extremely unlikely that a vocab list generated from 20,000 reviews (say) as opposed to 25,000 would be that different.\n",
    "\n",
    "The function below was intended simply to concatenate the reduced matrices for nose, palate and finish together, and keep track of what each column meant by creating a 'tasting note list'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a70107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_review_by_tasting_note_matrix(lowerbound=0):\n",
    "    \n",
    "    # create smaller matrices for each in turn with a lowerbound set to lowerbound. \n",
    "    lb = lowerbound\n",
    "    nose_matrix, nose_list = create_smaller_matrices(X_nose, lowerbound=lb, wordlist=wordlist)\n",
    "    palate_matrix, palate_list = create_smaller_matrices(X_palate, lowerbound=lb, wordlist=wordlist)\n",
    "    finish_matrix, finish_list = create_smaller_matrices(X_finish, lowerbound=lb, wordlist=wordlist)\n",
    "    \n",
    "    # Overlapping areas in terms of tasting notes between the three domains:\n",
    "    overlap_list = sorted((set(nose_list) & set(palate_list)) & set(finish_list))\n",
    "    \n",
    "    # Hstack the three matrices\n",
    "    review_by_tasting_note_matrix = np.hstack((nose_matrix, palate_matrix, finish_matrix))\n",
    "    \n",
    "    # Create tasting_note_list\n",
    "    nose_list = ['nose_'+item for item in nose_list]\n",
    "    palate_list = ['palate_'+item for item in palate_list]\n",
    "    finish_list = ['finish_'+item for item in palate_list]\n",
    "    tasting_note_list = nose_list+palate_list+finish_list\n",
    "    \n",
    "    return review_by_tasting_note_matrix, tasting_note_list, overlap_list\n",
    "\n",
    "# review_by_tasting_note_matrix, tasting_note_list, overlap_list = create_review_by_tasting_note_matrix(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0509f7f",
   "metadata": {},
   "source": [
    "At this point I realised this wasn't the right approach:\n",
    "\n",
    "1. You shouldn't be eliminating columns with a few hits in them anyway.\n",
    "2. Even deleting empty columns is basically a waste of time because of the way sparse matrices are handled by sklearn and scipy.\n",
    "3. I needed to write a pipeline so that I could start trying out classifiers, using cross-validation and fine-tuning, etc., and my functions weren't going to play nice inside a pipeline.\n",
    "\n",
    "---\n",
    "### New Approach\n",
    "\n",
    "I want to set up a pipeline now without any of this matrix reduction stuff, but simply employing three count-vectorisers in parallel for the nose, palate and finish respectively. I don't think there's going to be any way here to keep track of what each column means, but that's OK. Once the model is created and tuned and we know exactly what we want, we can always go back and build it manually and interpret it.\n",
    "\n",
    "So we need three tiny, bespoke functions that simply grabs the correct column of data. Then we run three countvectorizers in parallel, then we bring them together using featureunion, run them through a TfIDF transformer, and then a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a300b9",
   "metadata": {},
   "source": [
    "Below is an earlier pipeline I used to try out different classifiers. It was possible to get a small improvement by not using my vocabulary list, but that is effectively cheating since the review will often have the name of the whisky itself in the review, but I was interested to see how much difference it made. \n",
    "\n",
    "I tried out a whole bunch of classifiers with default values to get a sense of which ones would be best to have a go at fine-tuning. SVC, logistic regression and random forest came out well, but SGD and MLP also did fairly well. But because I didn't want a grid search taking forever, I ended up just looking for the best version of SVC, logistic regression and random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91cb64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for getting the right column of data to use in the pipeline:\n",
    "def getnose(array):\n",
    "    return array[:,0]\n",
    "    \n",
    "def getpalate(array):\n",
    "    return array[:,1]\n",
    "\n",
    "def getfinish(array):\n",
    "    return array[:,2]\n",
    "\n",
    "# Earlier pipeline I used to try out different classifiers.\n",
    "pipeline = Pipeline([\n",
    "        \n",
    "        ('union', FeatureUnion([\n",
    "            \n",
    "            ('pipe_nose', Pipeline([\n",
    "                ('get_nose', FunctionTransformer(getnose)),\n",
    "                #('countvec_nose', CountVectorizer(\n",
    "                #    tokenizer=tokenise_and_stem_text) )\n",
    "                ('countvec_nose', CountVectorizer(\n",
    "                    tokenizer=tokenise_and_stem_text, vocabulary=wordlist) )\n",
    "            ]) ),\n",
    "            ('pipe_palate', Pipeline([\n",
    "                ('get_palate', FunctionTransformer(getpalate)),\n",
    "                #('countvec_palate', CountVectorizer(\n",
    "                #    tokenizer=tokenise_and_stem_text) )\n",
    "                ('countvec_palate', CountVectorizer(\n",
    "                    tokenizer=tokenise_and_stem_text, vocabulary=wordlist) )\n",
    "            ]) ),\n",
    "            ('pipe_finish', Pipeline([\n",
    "                ('get_finish', FunctionTransformer(getfinish)),\n",
    "                #('countvec_finish', CountVectorizer(\n",
    "                #    tokenizer=tokenise_and_stem_text) )\n",
    "                ('countvec_finish', CountVectorizer(\n",
    "                    tokenizer=tokenise_and_stem_text, vocabulary=wordlist) )\n",
    "            ]) ),\n",
    "        ]) ),\n",
    "         \n",
    "        ('tfidf_transform', TfidfTransformer()),\n",
    "        \n",
    "        # Classifiers to try\n",
    "        ('clf_RF', RandomForestClassifier(n_jobs=-1)) #19%\n",
    "        #('clf_kneigh', KNeighborsClassifier()), #6%\n",
    "        #('clf_svc', SVC()), #19%\n",
    "        #('clf_gpc', GaussianProcessClassifier()), #didn't work\n",
    "        #('clf_tree', DecisionTreeClassifier()), #14%\n",
    "        #('clf_MLP', MLPClassifier()), #16%\n",
    "        #('clf_Ada', AdaBoostClassifier()), #4%\n",
    "        #('clf_NBayes', GaussianNB()), #didn't work\n",
    "        #('clf_QuadDA', QuadraticDiscriminantAnalysis()) #didn't work\n",
    "        #('clf_logreg', LogisticRegression()) #17%\n",
    "        #('clf_SGDclf', SGDClassifier()) #15%\n",
    "        ])\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df[['nose', 'palate', 'finish']], df['brand'])\n",
    "#X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
    "\n",
    "#pipeline.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = pipeline.predict(X_test)\n",
    "\n",
    "#print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc968591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=liblinear;, score=0.153 total time=  42.5s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=liblinear;, score=0.154 total time=  42.0s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=liblinear;, score=0.154 total time=  43.0s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=sag;, score=0.155 total time=  40.5s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=sag;, score=0.155 total time=  41.3s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=sag;, score=0.155 total time=  46.0s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=liblinear;, score=0.161 total time=  48.0s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=liblinear;, score=0.163 total time=  48.4s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=liblinear;, score=0.163 total time=  45.6s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=sag;, score=0.162 total time=  44.1s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=sag;, score=0.164 total time=  45.0s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=sag;, score=0.166 total time=  45.0s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=liblinear;, score=0.165 total time=  45.3s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=liblinear;, score=0.167 total time=  46.3s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=liblinear;, score=0.169 total time=  44.8s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=sag;, score=0.166 total time=  46.5s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=sag;, score=0.168 total time=  46.5s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=sag;, score=0.169 total time=  46.1s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=liblinear;, score=0.165 total time=  47.5s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=liblinear;, score=0.166 total time=  46.9s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=liblinear;, score=0.169 total time=  47.2s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=sag;, score=0.167 total time=  52.2s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=sag;, score=0.168 total time=  52.8s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=sag;, score=0.171 total time=  54.4s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=16, clf__penalty=l2, clf__solver=liblinear;, score=0.158 total time=  53.1s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=16, clf__penalty=l2, clf__solver=liblinear;, score=0.165 total time=  52.5s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=16, clf__penalty=l2, clf__solver=liblinear;, score=0.165 total time=  53.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamie\\anaconda3\\envs\\whisky_env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END clf=LogisticRegression(), clf__C=16, clf__penalty=l2, clf__solver=sag;, score=0.162 total time=  53.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamie\\anaconda3\\envs\\whisky_env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END clf=LogisticRegression(), clf__C=16, clf__penalty=l2, clf__solver=sag;, score=0.166 total time=  53.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamie\\anaconda3\\envs\\whisky_env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END clf=LogisticRegression(), clf__C=16, clf__penalty=l2, clf__solver=sag;, score=0.168 total time=  53.7s\n",
      "[CV 1/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.182 total time= 3.4min\n",
      "[CV 2/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.186 total time= 3.4min\n",
      "[CV 3/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.185 total time= 3.4min\n",
      "[CV 1/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.182 total time= 8.9min\n",
      "[CV 2/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.183 total time= 9.3min\n",
      "[CV 3/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.186 total time= 9.2min\n",
      "[CV 1/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.184 total time= 3.9min\n",
      "[CV 2/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.185 total time= 3.7min\n",
      "[CV 3/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.188 total time= 3.6min\n",
      "[CV 1/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.181 total time= 9.4min\n",
      "[CV 2/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.185 total time= 9.8min\n",
      "[CV 3/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.186 total time= 9.3min\n",
      "[CV 1/3] END clf=SVC(), clf__C=2, clf__degree=1;, score=0.195 total time= 3.5min\n",
      "[CV 2/3] END clf=SVC(), clf__C=2, clf__degree=1;, score=0.190 total time= 3.4min\n",
      "[CV 3/3] END clf=SVC(), clf__C=2, clf__degree=1;, score=0.194 total time= 3.4min\n",
      "[CV 1/3] END clf=SVC(), clf__C=2, clf__degree=2;, score=0.195 total time= 3.3min\n",
      "[CV 2/3] END clf=SVC(), clf__C=2, clf__degree=2;, score=0.190 total time= 3.3min\n",
      "[CV 3/3] END clf=SVC(), clf__C=2, clf__degree=2;, score=0.194 total time= 3.3min\n",
      "[CV 1/3] END clf=SVC(), clf__C=3, clf__degree=1;, score=0.192 total time= 3.3min\n",
      "[CV 2/3] END clf=SVC(), clf__C=3, clf__degree=1;, score=0.189 total time= 3.3min\n",
      "[CV 3/3] END clf=SVC(), clf__C=3, clf__degree=1;, score=0.194 total time= 3.3min\n",
      "[CV 1/3] END clf=SVC(), clf__C=3, clf__degree=2;, score=0.192 total time= 3.4min\n",
      "[CV 2/3] END clf=SVC(), clf__C=3, clf__degree=2;, score=0.189 total time= 3.2min\n",
      "[CV 3/3] END clf=SVC(), clf__C=3, clf__degree=2;, score=0.194 total time= 3.2min\n",
      "[CV 1/3] END clf=SVC(), clf__C=4, clf__degree=1;, score=0.192 total time= 3.2min\n",
      "[CV 2/3] END clf=SVC(), clf__C=4, clf__degree=1;, score=0.189 total time= 3.2min\n",
      "[CV 3/3] END clf=SVC(), clf__C=4, clf__degree=1;, score=0.194 total time= 3.2min\n",
      "[CV 1/3] END clf=SVC(), clf__C=4, clf__degree=2;, score=0.192 total time= 3.3min\n",
      "[CV 2/3] END clf=SVC(), clf__C=4, clf__degree=2;, score=0.189 total time= 3.2min\n",
      "[CV 3/3] END clf=SVC(), clf__C=4, clf__degree=2;, score=0.194 total time= 3.3min\n",
      "                                         precision    recall  f1-score   support\n",
      "\n",
      "                                   1792       1.00      0.15      0.26        20\n",
      "                              Aberfeldy       0.00      0.00      0.00        13\n",
      "                               Aberlour       0.18      0.19      0.18       114\n",
      "                              Ailsa Bay       0.00      0.00      0.00         4\n",
      "                                 Akashi       0.00      0.00      0.00         9\n",
      "                                Alberta       1.00      0.08      0.15        12\n",
      "                         Allt-A-Bhainne       0.00      0.00      0.00         5\n",
      "                                  Amrut       0.39      0.14      0.20        66\n",
      "                                 AnCnoc       0.11      0.03      0.05        29\n",
      "                            Angels Envy       0.00      0.00      0.00         8\n",
      "                        Angels Envy Rye       0.50      0.67      0.57         3\n",
      "                                 Ardbeg       0.15      0.40      0.22       184\n",
      "                                Ardmore       0.50      0.09      0.15        33\n",
      "                           Ardnamurchan       0.00      0.00      0.00         3\n",
      "                                  Arran       0.11      0.24      0.15       113\n",
      "                           Auchentoshan       0.24      0.17      0.20        60\n",
      "                              Auchroisk       0.00      0.00      0.00        11\n",
      "                               Aultmore       0.00      0.00      0.00        16\n",
      "                                 Bakers       0.50      0.17      0.25         6\n",
      "                               Balblair       1.00      0.06      0.11        18\n",
      "                               Balcones       1.00      0.08      0.15        24\n",
      "                            Ballantines       0.00      0.00      0.00        12\n",
      "                              Ballechin       0.00      0.00      0.00        18\n",
      "                              Balmenach       0.00      0.00      0.00         7\n",
      "                               Balvenie       0.17      0.42      0.24       141\n",
      "                        Barrell Whiskey       0.00      0.00      0.00         3\n",
      "                          Basil Haydens       0.00      0.00      0.00         3\n",
      "                              Ben Nevis       0.71      0.14      0.23        36\n",
      "                               BenRiach       0.24      0.13      0.17        82\n",
      "                              Benrinnes       0.40      0.08      0.14        48\n",
      "                              Benromach       0.75      0.08      0.14        38\n",
      "                           Black Bottle       1.00      0.44      0.62         9\n",
      "                           Black Grouse       0.00      0.00      0.00         2\n",
      "                               Bladnoch       0.50      0.16      0.24        19\n",
      "                            Blair Athol       1.00      0.08      0.15        12\n",
      "                               Blantons       0.78      0.29      0.42        24\n",
      "                                Bookers       0.58      0.38      0.46        29\n",
      "                            Bookers Rye       0.00      0.00      0.00         1\n",
      "                                Bowmore       0.17      0.10      0.13       112\n",
      "                                Braeval       0.00      0.00      0.00         2\n",
      "                                  Brora       0.50      0.07      0.12        14\n",
      "                          Bruichladdich       0.14      0.37      0.20       189\n",
      "                              Buchanans       0.00      0.00      0.00         4\n",
      "                          Buffalo Trace       0.44      0.15      0.22        27\n",
      "                        Bulleit Bourbon       0.00      0.00      0.00        13\n",
      "                            Bulleit Rye       0.00      0.00      0.00        10\n",
      "                           Bunnahabhain       0.13      0.27      0.17       176\n",
      "                              Bushmills       1.00      0.09      0.17        11\n",
      "                            Campbeltown       0.00      0.00      0.00         7\n",
      "                          Canadian Club       0.00      0.00      0.00         9\n",
      "                               Caol Ila       0.25      0.29      0.27       145\n",
      "                            Caperdonich       0.00      0.00      0.00         3\n",
      "                                 Cardhu       0.50      0.11      0.18         9\n",
      "                               Chichibu       0.00      0.00      0.00         1\n",
      "                           Chivas Regal       0.00      0.00      0.00        19\n",
      "                              Clynelish       0.33      0.38      0.35        72\n",
      "                      Colonel EH Taylor       0.12      0.06      0.08        34\n",
      "         Colonel EH Taylor Straight Rye       0.00      0.00      0.00        11\n",
      "                      Compass Box Asyla       0.00      0.00      0.00         3\n",
      "              Compass Box Flaming Heart       1.00      0.15      0.27        13\n",
      "                 Compass Box Great King       0.25      0.05      0.08        20\n",
      "                   Compass Box Hedonism       0.00      0.00      0.00         7\n",
      "                    Compass Box No Name       0.00      0.00      0.00         2\n",
      "                  Compass Box Oak Cross       0.00      0.00      0.00         9\n",
      "               Compass Box Peat Monster       0.00      0.00      0.00        15\n",
      "                 Compass Box Spice Tree       0.00      0.00      0.00         8\n",
      "Compass Box This Is Not a Luxury Whisky       0.00      0.00      0.00         5\n",
      "                              Connemara       0.00      0.00      0.00         7\n",
      "                                 Cooley       0.00      0.00      0.00         4\n",
      "                            Cragganmore       1.00      0.05      0.10        20\n",
      "                          Craigellachie       0.50      0.04      0.07        26\n",
      "                            Crown Royal       0.00      0.00      0.00        15\n",
      "                             Cutty Sark       0.00      0.00      0.00         8\n",
      "                               Daftmill       0.00      0.00      0.00         3\n",
      "                              Dailuaine       1.00      0.07      0.12        15\n",
      "                             Dallas Dhu       0.00      0.00      0.00         2\n",
      "                                Dalmore       0.57      0.10      0.17        40\n",
      "                             Dalwhinnie       0.67      0.16      0.26        25\n",
      "                               Deanston       1.00      0.11      0.19        19\n",
      "                                 Dewars       0.00      0.00      0.00         9\n",
      "                               Dufftown       1.00      0.14      0.25         7\n",
      "                             Eagle Rare       0.45      0.13      0.20        38\n",
      "                               Edradour       0.50      0.16      0.24        31\n",
      "                           Elijah Craig       0.18      0.40      0.25        57\n",
      "                            Elmer T Lee       0.00      0.00      0.00        10\n",
      "                          Evan Williams       0.25      0.17      0.20        35\n",
      "                          Famous Grouse       1.00      0.08      0.15        12\n",
      "                            Fettercairn       0.00      0.00      0.00        13\n",
      "                          Fighting Cock       0.00      0.00      0.00         3\n",
      "                              Finlaggan       0.00      0.00      0.00         4\n",
      "                            Forty Creek       0.00      0.00      0.00        13\n",
      "                             Four Roses       0.15      0.63      0.24       119\n",
      "                          George Dickel       1.00      0.14      0.25        14\n",
      "                         George T Stagg       0.50      0.12      0.20        16\n",
      "                            Glen Breton       1.00      0.30      0.46        10\n",
      "                             Glen Elgin       0.00      0.00      0.00        14\n",
      "                           Glen Garioch       1.00      0.13      0.23        23\n",
      "                             Glen Grant       1.00      0.06      0.11        34\n",
      "                             Glen Keith       0.00      0.00      0.00         6\n",
      "                             Glen Moray       0.50      0.07      0.13        27\n",
      "                               Glen Ord       0.00      0.00      0.00         7\n",
      "                            Glen Scotia       0.60      0.10      0.17        31\n",
      "                              Glen Spey       0.00      0.00      0.00         4\n",
      "                           Glenallachie       1.00      0.07      0.12        15\n",
      "                             Glenburgie       0.00      0.00      0.00        20\n",
      "                              Glencadam       0.00      0.00      0.00        11\n",
      "                            Glendronach       0.18      0.44      0.25       147\n",
      "                             Glendullan       0.00      0.00      0.00         6\n",
      "                            Glenfarclas       0.23      0.22      0.23       124\n",
      "                            Glenfiddich       0.16      0.33      0.22        85\n",
      "                          Glenglassaugh       0.00      0.00      0.00        13\n",
      "                              Glengoyne       1.00      0.02      0.04        48\n",
      "                            Glenkinchie       0.00      0.00      0.00        15\n",
      "                              Glenlivet       0.11      0.24      0.15       121\n",
      "                             Glenlossie       0.00      0.00      0.00         6\n",
      "                           Glenmorangie       0.13      0.29      0.18       124\n",
      "                             Glenrothes       0.40      0.05      0.09        41\n",
      "                           Glentauchers       0.25      0.06      0.10        16\n",
      "                             Glenturret       1.00      0.07      0.12        15\n",
      "                      Gooderham & Worts       0.00      0.00      0.00         6\n",
      "                             Green Spot       0.00      0.00      0.00         8\n",
      "                                Hakushu       0.75      0.27      0.40        11\n",
      "                              Hazelburn       0.50      0.05      0.09        20\n",
      "                              Heartwood       1.00      0.10      0.18        10\n",
      "                            Heaven Hill       0.00      0.00      0.00        12\n",
      "                          Hellyers Road       0.00      0.00      0.00         3\n",
      "                          Henry McKenna       0.67      0.17      0.27        12\n",
      "                                 Hibiki       0.00      0.00      0.00        10\n",
      "                              High West       0.00      0.00      0.00        14\n",
      "                          Highland Park       0.16      0.26      0.20       146\n",
      "                                Ichiros       0.00      0.00      0.00         7\n",
      "                                 Ileach       0.00      0.00      0.00         2\n",
      "                               Imperial       1.00      0.07      0.13        14\n",
      "                              Inchgower       1.00      0.20      0.33         5\n",
      "                             Inchmurrin       0.00      0.00      0.00         5\n",
      "                               JP Wiser       0.20      0.05      0.08        19\n",
      "                           Jack Daniels       0.30      0.30      0.30        23\n",
      "                         James E Pepper       0.00      0.00      0.00         3\n",
      "                                Jameson       0.00      0.00      0.00        21\n",
      "                             Jeffersons       1.00      0.12      0.21        17\n",
      "                               Jim Beam       0.27      0.25      0.26        16\n",
      "                         Johnnie Walker       0.19      0.33      0.24        80\n",
      "                                   Jura       0.12      0.03      0.04        39\n",
      "                              Karuizawa       0.00      0.00      0.00         6\n",
      "                                Kavalan       0.62      0.22      0.32        37\n",
      "                       Kentucky Owl Rye       0.00      0.00      0.00         2\n",
      "                              Kilchoman       0.46      0.24      0.32       107\n",
      "                              Kilkerran       0.90      0.32      0.47        28\n",
      "                       Knappogue Castle       1.00      0.50      0.67         2\n",
      "                             Knob Creek       0.19      0.15      0.17        33\n",
      "                         Knob Creek Rye       0.00      0.00      0.00         4\n",
      "                              Knockando       0.00      0.00      0.00         3\n",
      "                              Lagavulin       0.32      0.19      0.24       103\n",
      "                              Laphroaig       0.33      0.56      0.41       272\n",
      "                                Larceny       1.00      0.17      0.29         6\n",
      "                                   Lark       0.50      0.25      0.33         4\n",
      "                                 Ledaig       0.52      0.16      0.25        86\n",
      "                               Linkwood       0.00      0.00      0.00        29\n",
      "                                Lismore       0.00      0.00      0.00         3\n",
      "                             Littlemill       0.00      0.00      0.00         7\n",
      "                            Loch Lomond       0.00      0.00      0.00         7\n",
      "                               Longmorn       0.00      0.00      0.00        27\n",
      "                                Longrow       0.33      0.11      0.17        62\n",
      "                              Lot No 40       0.75      0.30      0.43        10\n",
      "                               Macallan       0.13      0.31      0.18       110\n",
      "                  Mackinlays Shackleton       1.00      0.12      0.22         8\n",
      "                            Makers Mark       0.18      0.13      0.15        45\n",
      "                            Mannochmore       0.00      0.00      0.00         3\n",
      "                              McCarthys       0.00      0.00      0.00         6\n",
      "                            Mellow Corn       0.00      0.00      0.00         2\n",
      "                               Michters       0.00      0.00      0.00        12\n",
      "                           Michters Rye       0.00      0.00      0.00        11\n",
      "                               Midleton       0.00      0.00      0.00         5\n",
      "                             Miltonduff       1.00      0.14      0.25         7\n",
      "                        Monkey Shoulder       0.00      0.00      0.00         4\n",
      "                               Mortlach       0.40      0.07      0.12        59\n",
      "                               New Riff       1.00      0.25      0.40         4\n",
      "                                  Nikka       0.73      0.16      0.26        50\n",
      "                             Noahs Mill       1.00      0.14      0.25         7\n",
      "                                   Oban       0.00      0.00      0.00        43\n",
      "                               Octomore       0.35      0.10      0.15        71\n",
      "                            Old Charter       0.00      0.00      0.00         6\n",
      "                               Old Crow       0.00      0.00      0.00         6\n",
      "                               Old Ezra       0.00      0.00      0.00         6\n",
      "                         Old Fitzgerald       1.00      0.09      0.17        11\n",
      "                           Old Forester       0.62      0.14      0.22        37\n",
      "                          Old Grand Dad       0.60      0.10      0.17        31\n",
      "                           Old Overholt       0.00      0.00      0.00         2\n",
      "                              Old Perth       1.00      0.20      0.33         5\n",
      "                           Old Pulteney       0.26      0.12      0.16        51\n",
      "                     Old Rip Van Winkle       0.00      0.00      0.00         6\n",
      "                     Old Weller Antique       0.37      0.22      0.27        32\n",
      "                                   Omar       0.00      0.00      0.00         3\n",
      "                          Orphan Barrel       1.00      0.06      0.11        18\n",
      "                              Paul John       0.00      0.00      0.00        11\n",
      "                              Pigs Nose       0.00      0.00      0.00         1\n",
      "                             Pike Creek       0.00      0.00      0.00         6\n",
      "                             Pikesville       1.00      0.20      0.33        10\n",
      "                             Pittyvaich       0.00      0.00      0.00         3\n",
      "                            Port Askaig       0.00      0.00      0.00         2\n",
      "                         Port Charlotte       0.34      0.14      0.20        70\n",
      "                             Port Ellen       0.00      0.00      0.00        16\n",
      "                                 Powers       0.00      0.00      0.00         1\n",
      "                             Rebel Yell       0.00      0.00      0.00         6\n",
      "                              Redbreast       1.00      0.04      0.08        23\n",
      "                             Redemption       0.00      0.00      0.00         5\n",
      "                            Rittenhouse       0.00      0.00      0.00        11\n",
      "                        Rock Hill Farms       1.00      0.20      0.33         5\n",
      "                               Rosebank       0.00      0.00      0.00         8\n",
      "                           Rowans Creek       0.00      0.00      0.00         2\n",
      "                          Royal Brackla       0.00      0.00      0.00         5\n",
      "                        Royal Lochnagar       1.00      0.20      0.33        10\n",
      "                       Russells Reserve       0.57      0.10      0.17        39\n",
      "                   Russells Reserve Rye       1.00      0.33      0.50         6\n",
      "                                Sazerac       0.14      0.05      0.07        20\n",
      "                                  Scapa       0.00      0.00      0.00        10\n",
      "                              Sheep Dip       0.00      0.00      0.00         4\n",
      "                          Smooth Ambler       0.00      0.00      0.00         9\n",
      "                               Speyburn       0.00      0.00      0.00         5\n",
      "                             Springbank       0.17      0.27      0.21       128\n",
      "                               Stagg Jr       0.09      0.05      0.06        21\n",
      "                           Still Waters       1.00      0.20      0.33         5\n",
      "                             Stranahans       1.00      1.00      1.00         2\n",
      "                             Strathisla       0.00      0.00      0.00         7\n",
      "                             Strathmill       0.50      0.14      0.22         7\n",
      "                             Stronachie       0.00      0.00      0.00         1\n",
      "                         Sullivans Cove       1.00      0.33      0.50         6\n",
      "                               Talisker       0.35      0.33      0.34       109\n",
      "                                 Tamdhu       0.00      0.00      0.00        22\n",
      "                             Tamnavulin       0.00      0.00      0.00         3\n",
      "                               Teachers       0.00      0.00      0.00         2\n",
      "                                Teeling       1.00      0.25      0.40        12\n",
      "                         Thomas H Handy       0.20      0.06      0.10        16\n",
      "                              Tobermory       1.00      0.03      0.07        29\n",
      "                                Tomatin       0.67      0.09      0.16        43\n",
      "                              Tomintoul       1.00      0.23      0.38        13\n",
      "                                Tormore       0.00      0.00      0.00         6\n",
      "                                Toronto       1.00      0.33      0.50         6\n",
      "                          Tullamore Dew       0.00      0.00      0.00         2\n",
      "                           Tullibardine       0.00      0.00      0.00        10\n",
      "                             Tyrconnell       0.00      0.00      0.00         6\n",
      "                             Van Winkle       0.57      0.17      0.27        23\n",
      "                        Very Old Barton       0.50      0.33      0.40         3\n",
      "                              WL Weller       0.33      0.09      0.15        32\n",
      "                               Westland       1.00      0.06      0.12        16\n",
      "                             WhistlePig       0.31      0.48      0.38        27\n",
      "                            Wild Turkey       0.22      0.34      0.27        87\n",
      "                    Wild Turkey 101 Rye       0.00      0.00      0.00         6\n",
      "                                Willett       0.75      0.15      0.25        20\n",
      "              Willett Family Estate Rye       0.29      0.14      0.19        29\n",
      "                       Woodford Reserve       0.40      0.13      0.20        15\n",
      "                          Writers Tears       0.00      0.00      0.00         2\n",
      "                               Yamazaki       1.00      0.11      0.19        19\n",
      "                            Yellow Spot       0.00      0.00      0.00         1\n",
      "\n",
      "                               accuracy                           0.21      6739\n",
      "                              macro avg       0.32      0.10      0.13      6739\n",
      "                           weighted avg       0.32      0.21      0.19      6739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for getting the right column of data to use in the pipeline:\n",
    "def getnose(array):\n",
    "    return array[:,0]\n",
    "    \n",
    "def getpalate(array):\n",
    "    return array[:,1]\n",
    "\n",
    "def getfinish(array):\n",
    "    return array[:,2]\n",
    "\n",
    "# Pipeline to use\n",
    "pipeline = Pipeline([\n",
    "        \n",
    "        ('union', FeatureUnion([\n",
    "            \n",
    "            ('pipe_nose', Pipeline([\n",
    "                ('get_nose', FunctionTransformer(getnose)),\n",
    "                ('countvec_nose', CountVectorizer(\n",
    "                    tokenizer=tokenise_and_stem_text, vocabulary=wordlist) )\n",
    "            ]) ),\n",
    "            ('pipe_palate', Pipeline([\n",
    "                ('get_palate', FunctionTransformer(getpalate)),\n",
    "                ('countvec_palate', CountVectorizer(\n",
    "                    tokenizer=tokenise_and_stem_text, vocabulary=wordlist) )\n",
    "            ]) ),\n",
    "            ('pipe_finish', Pipeline([\n",
    "                ('get_finish', FunctionTransformer(getfinish)),\n",
    "                ('countvec_finish', CountVectorizer(\n",
    "                    tokenizer=tokenise_and_stem_text, vocabulary=wordlist) )\n",
    "            ]) ),\n",
    "        ]) ),\n",
    "         \n",
    "        ('tfidf_transform', TfidfTransformer()),\n",
    "        \n",
    "        ('clf', RandomForestClassifier(n_jobs=-1))\n",
    "        ])\n",
    "\n",
    "param_grid = [\n",
    "    {'clf' : [LogisticRegression()],\n",
    "     'clf__penalty' : ['l2'],\n",
    "    'clf__C' : [1, 2, 4, 8],\n",
    "    'clf__solver' : ['liblinear', 'sag']},\n",
    "    {'clf' : [RandomForestClassifier(n_jobs=-1)],\n",
    "    'clf__bootstrap': [False],\n",
    "    'clf__max_depth': [200, 500],\n",
    "    'clf__max_features': ['sqrt'],\n",
    "    'clf__n_estimators': [1000, 2000]},\n",
    "    {'clf' : [SVC()],\n",
    "     'clf__degree' : [1, 2],\n",
    "     'clf__C' : [2, 3, 4]}\n",
    "]\n",
    "\n",
    "# First of all, do the whole set of whiskies\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['nose', 'palate', 'finish']], df['brand'])\n",
    "X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
    "\n",
    "grid_search_all_whiskies = GridSearchCV(estimator = pipeline, cv=3, \n",
    "                    param_grid = param_grid, scoring='accuracy', verbose=3)\n",
    "\n",
    "grid_search_all_whiskies.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grid_search_all_whiskies.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0c70bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to pickle file\n",
    "with open('./models/whisky_classifier.pkl', \"wb\") as f:\n",
    "    joblib.dump(grid_search_all_whiskies, f, compress='zlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c818a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         precision    recall  f1-score   support\n",
      "\n",
      "                                   1792       1.00      0.97      0.99        72\n",
      "                              Aberfeldy       1.00      0.93      0.97        46\n",
      "                               Aberlour       0.97      0.96      0.97       317\n",
      "                              Ailsa Bay       1.00      1.00      1.00         7\n",
      "                                 Akashi       1.00      1.00      1.00        15\n",
      "                                Alberta       1.00      1.00      1.00        30\n",
      "                         Allt-A-Bhainne       1.00      1.00      1.00         9\n",
      "                                  Amrut       0.99      0.98      0.99       169\n",
      "                                 AnCnoc       0.98      0.89      0.93        93\n",
      "                            Angels Envy       1.00      0.91      0.95        23\n",
      "                        Angels Envy Rye       1.00      1.00      1.00        16\n",
      "                                 Ardbeg       0.73      0.99      0.84       628\n",
      "                                Ardmore       0.98      0.96      0.97       134\n",
      "                           Ardnamurchan       1.00      1.00      1.00        12\n",
      "                                  Arran       0.95      0.96      0.96       314\n",
      "                           Auchentoshan       0.97      0.96      0.97       162\n",
      "                              Auchroisk       1.00      0.93      0.96        29\n",
      "                               Aultmore       1.00      0.94      0.97        52\n",
      "                                 Bakers       1.00      0.91      0.95        23\n",
      "                               Balblair       0.99      0.97      0.98        77\n",
      "                               Balcones       1.00      1.00      1.00        44\n",
      "                            Ballantines       1.00      0.90      0.95        10\n",
      "                              Ballechin       1.00      0.98      0.99        64\n",
      "                              Balmenach       1.00      0.79      0.88        14\n",
      "                               Balvenie       0.95      0.98      0.97       399\n",
      "                        Barrell Whiskey       1.00      1.00      1.00         8\n",
      "                          Basil Haydens       1.00      1.00      1.00        13\n",
      "                              Ben Nevis       1.00      0.95      0.97       116\n",
      "                               BenRiach       0.99      0.97      0.98       255\n",
      "                              Benrinnes       0.96      0.97      0.96       150\n",
      "                              Benromach       0.98      0.97      0.98       113\n",
      "                           Black Bottle       1.00      1.00      1.00        21\n",
      "                           Black Grouse       1.00      1.00      1.00         9\n",
      "                               Bladnoch       0.94      0.98      0.96        45\n",
      "                            Blair Athol       1.00      0.96      0.98        51\n",
      "                               Blantons       0.97      0.95      0.96        96\n",
      "                                Bookers       0.99      0.95      0.97        88\n",
      "                            Bookers Rye       1.00      0.80      0.89        10\n",
      "                                Bowmore       0.99      0.97      0.98       299\n",
      "                                Braeval       1.00      0.93      0.96        14\n",
      "                                  Brora       1.00      0.95      0.97        41\n",
      "                          Bruichladdich       0.96      0.98      0.97       485\n",
      "                              Buchanans       1.00      1.00      1.00         7\n",
      "                          Buffalo Trace       0.98      0.95      0.97        65\n",
      "                        Bulleit Bourbon       0.82      0.93      0.87        40\n",
      "                            Bulleit Rye       0.96      0.96      0.96        26\n",
      "                           Bunnahabhain       0.97      0.97      0.97       511\n",
      "                              Bushmills       1.00      0.98      0.99        41\n",
      "                            Campbeltown       1.00      0.86      0.92        14\n",
      "                          Canadian Club       1.00      1.00      1.00        31\n",
      "                               Caol Ila       0.98      0.98      0.98       393\n",
      "                            Caperdonich       1.00      0.87      0.93        15\n",
      "                                 Cardhu       1.00      0.93      0.96        29\n",
      "                               Chichibu       1.00      0.91      0.95        11\n",
      "                           Chivas Regal       1.00      0.96      0.98        45\n",
      "                              Clynelish       0.98      0.98      0.98       239\n",
      "                      Colonel EH Taylor       0.98      0.99      0.99       110\n",
      "         Colonel EH Taylor Straight Rye       1.00      0.83      0.90        23\n",
      "                      Compass Box Asyla       1.00      0.67      0.80        12\n",
      "              Compass Box Flaming Heart       1.00      0.93      0.96        27\n",
      "                 Compass Box Great King       0.78      0.95      0.86        42\n",
      "                   Compass Box Hedonism       0.88      0.88      0.88        26\n",
      "                    Compass Box No Name       1.00      0.95      0.97        19\n",
      "                  Compass Box Oak Cross       1.00      0.88      0.93        16\n",
      "               Compass Box Peat Monster       0.96      0.80      0.87        30\n",
      "                 Compass Box Spice Tree       0.89      0.81      0.85        31\n",
      "Compass Box This Is Not a Luxury Whisky       1.00      0.90      0.95        10\n",
      "                              Connemara       1.00      1.00      1.00        15\n",
      "                                 Cooley       1.00      1.00      1.00         8\n",
      "                            Cragganmore       1.00      0.99      0.99        68\n",
      "                          Craigellachie       1.00      0.95      0.98        85\n",
      "                            Crown Royal       1.00      0.98      0.99        54\n",
      "                             Cutty Sark       1.00      1.00      1.00        22\n",
      "                               Daftmill       1.00      1.00      1.00        12\n",
      "                              Dailuaine       1.00      0.95      0.97        37\n",
      "                             Dallas Dhu       1.00      0.90      0.95        10\n",
      "                                Dalmore       1.00      0.97      0.98       125\n",
      "                             Dalwhinnie       0.98      0.96      0.97        52\n",
      "                               Deanston       0.99      0.94      0.96        81\n",
      "                                 Dewars       1.00      0.93      0.96        44\n",
      "                                 Dimple       1.00      1.00      1.00        12\n",
      "                               Dufftown       1.00      1.00      1.00        29\n",
      "                             Eagle Rare       0.96      0.97      0.96        88\n",
      "                               Edradour       0.98      0.98      0.98       106\n",
      "                           Elijah Craig       0.95      0.99      0.97       201\n",
      "                            Elmer T Lee       1.00      0.89      0.94        38\n",
      "                          Evan Williams       0.95      0.96      0.96       107\n",
      "                          Famous Grouse       0.97      0.94      0.95        33\n",
      "                            Fettercairn       0.98      0.95      0.96        43\n",
      "                          Fighting Cock       1.00      1.00      1.00        12\n",
      "                              Finlaggan       1.00      1.00      1.00        25\n",
      "                            Forty Creek       1.00      1.00      1.00        34\n",
      "                             Four Roses       0.97      1.00      0.98       413\n",
      "                          George Dickel       1.00      0.94      0.97        32\n",
      "                         George T Stagg       0.94      0.94      0.94        63\n",
      "                            Glen Breton       1.00      1.00      1.00         7\n",
      "                             Glen Elgin       1.00      0.97      0.98        31\n",
      "                           Glen Garioch       0.97      0.95      0.96        78\n",
      "                             Glen Grant       0.94      0.97      0.96       104\n",
      "                             Glen Keith       0.96      0.90      0.93        29\n",
      "                             Glen Moray       0.99      0.99      0.99        79\n",
      "                               Glen Ord       1.00      0.95      0.98        21\n",
      "                            Glen Scotia       0.99      0.99      0.99       101\n",
      "                              Glen Spey       1.00      0.92      0.96        12\n",
      "                           Glenallachie       1.00      0.98      0.99        61\n",
      "                             Glenburgie       1.00      0.93      0.96        44\n",
      "                              Glencadam       1.00      0.93      0.96        29\n",
      "                            Glendronach       0.94      0.98      0.96       434\n",
      "                             Glendullan       1.00      0.86      0.92        21\n",
      "                            Glenfarclas       0.98      0.96      0.97       325\n",
      "                            Glenfiddich       0.98      0.97      0.97       292\n",
      "                          Glenglassaugh       1.00      0.96      0.98        25\n",
      "                              Glengoyne       0.99      0.99      0.99       130\n",
      "                            Glenkinchie       1.00      0.93      0.96        28\n",
      "                              Glenlivet       0.97      0.98      0.98       349\n",
      "                             Glenlossie       0.98      0.96      0.97        48\n",
      "                           Glenmorangie       0.96      0.95      0.96       374\n",
      "                             Glenrothes       0.98      0.94      0.96       117\n",
      "                           Glentauchers       1.00      0.92      0.96        48\n",
      "                             Glenturret       1.00      0.91      0.95        45\n",
      "                      Gooderham & Worts       1.00      1.00      1.00        12\n",
      "                             Green Spot       0.94      1.00      0.97        16\n",
      "                                Hakushu       1.00      0.94      0.97        35\n",
      "                              Hazelburn       1.00      0.95      0.97        60\n",
      "                              Heartwood       0.97      1.00      0.98        30\n",
      "                            Heaven Hill       1.00      0.88      0.94        34\n",
      "                          Hellyers Road       1.00      1.00      1.00        12\n",
      "                          Henry McKenna       0.98      0.96      0.97        47\n",
      "                                 Hibiki       0.95      0.98      0.96        42\n",
      "                              High West       1.00      0.94      0.97        35\n",
      "                          Highland Park       0.98      0.98      0.98       444\n",
      "                                Ichiros       1.00      1.00      1.00         8\n",
      "                                 Ileach       1.00      1.00      1.00         9\n",
      "                               Imperial       1.00      0.96      0.98        26\n",
      "                              Inchgower       1.00      1.00      1.00        22\n",
      "                             Inchmurrin       1.00      1.00      1.00        16\n",
      "                               JP Wiser       0.94      1.00      0.97        51\n",
      "                           Jack Daniels       1.00      0.92      0.96        87\n",
      "                         James E Pepper       1.00      1.00      1.00         9\n",
      "                                Jameson       0.98      0.98      0.98        52\n",
      "                             Jeffersons       1.00      0.96      0.98        28\n",
      "                               Jim Beam       1.00      1.00      1.00        84\n",
      "                         Johnnie Walker       0.98      0.96      0.97       300\n",
      "                                   Jura       0.93      0.97      0.95       131\n",
      "                              Karuizawa       1.00      1.00      1.00        11\n",
      "                                Kavalan       1.00      0.99      1.00       120\n",
      "                       Kentucky Owl Rye       1.00      0.90      0.95        10\n",
      "                              Kilchoman       1.00      0.95      0.97       274\n",
      "                              Kilkerran       0.98      0.98      0.98       114\n",
      "                       Knappogue Castle       1.00      1.00      1.00        17\n",
      "                             Knob Creek       0.99      0.99      0.99        99\n",
      "                         Knob Creek Rye       1.00      1.00      1.00        26\n",
      "                              Knockando       1.00      1.00      1.00        13\n",
      "                              Lagavulin       0.98      0.95      0.97       322\n",
      "                              Laphroaig       0.96      0.97      0.96       737\n",
      "                                Larceny       1.00      1.00      1.00        26\n",
      "                                   Lark       0.93      0.93      0.93        14\n",
      "                                 Ledaig       0.98      0.98      0.98       234\n",
      "                               Linkwood       0.99      0.97      0.98        79\n",
      "                                Lismore       1.00      0.90      0.95        10\n",
      "                             Littlemill       1.00      0.79      0.88        24\n",
      "                            Loch Lomond       1.00      1.00      1.00        19\n",
      "                               Longmorn       1.00      0.94      0.97        64\n",
      "                                Longrow       0.98      0.98      0.98       181\n",
      "                              Lot No 40       0.94      0.94      0.94        33\n",
      "                               Macallan       0.93      0.97      0.95       407\n",
      "                  Mackinlays Shackleton       1.00      0.94      0.97        18\n",
      "                            Makers Mark       0.98      1.00      0.99       110\n",
      "                            Mannochmore       1.00      0.89      0.94        19\n",
      "                              McCarthys       1.00      0.86      0.92         7\n",
      "                            Mellow Corn       1.00      1.00      1.00        16\n",
      "                               Michters       0.97      1.00      0.99        38\n",
      "                           Michters Rye       1.00      0.96      0.98        24\n",
      "                               Midleton       1.00      1.00      1.00         9\n",
      "                             Miltonduff       1.00      0.90      0.95        20\n",
      "                        Monkey Shoulder       1.00      0.96      0.98        28\n",
      "                               Mortlach       0.98      0.98      0.98       137\n",
      "                               New Riff       1.00      1.00      1.00        11\n",
      "                                  Nikka       0.99      0.99      0.99       140\n",
      "                             Noahs Mill       1.00      1.00      1.00        16\n",
      "                                   Oban       0.98      0.98      0.98        83\n",
      "                               Octomore       1.00      0.96      0.98       226\n",
      "                            Old Charter       1.00      1.00      1.00        11\n",
      "                               Old Crow       1.00      1.00      1.00         9\n",
      "                               Old Ezra       1.00      0.95      0.97        19\n",
      "                         Old Fitzgerald       1.00      1.00      1.00        28\n",
      "                           Old Forester       1.00      0.97      0.98        96\n",
      "                          Old Grand Dad       0.97      0.97      0.97        78\n",
      "                           Old Overholt       1.00      0.86      0.92        14\n",
      "                              Old Perth       1.00      1.00      1.00        14\n",
      "                           Old Pulteney       0.99      0.93      0.96       165\n",
      "                     Old Rip Van Winkle       1.00      0.86      0.93        29\n",
      "                     Old Weller Antique       0.94      0.91      0.93        93\n",
      "                                   Omar       1.00      1.00      1.00        11\n",
      "                          Orphan Barrel       1.00      0.94      0.97        49\n",
      "                              Paul John       1.00      1.00      1.00        23\n",
      "                              Pigs Nose       1.00      1.00      1.00        12\n",
      "                             Pike Creek       1.00      0.80      0.89        10\n",
      "                             Pikesville       1.00      0.95      0.98        22\n",
      "                             Pittyvaich       1.00      1.00      1.00         8\n",
      "                            Port Askaig       1.00      1.00      1.00        16\n",
      "                         Port Charlotte       0.97      0.96      0.96       249\n",
      "                             Port Ellen       0.98      0.98      0.98        46\n",
      "                                 Powers       1.00      0.92      0.96        12\n",
      "                             Rebel Yell       1.00      1.00      1.00        18\n",
      "                              Redbreast       1.00      0.97      0.99        74\n",
      "                             Redemption       1.00      1.00      1.00        13\n",
      "                            Rittenhouse       0.97      0.95      0.96        38\n",
      "                        Rock Hill Farms       1.00      0.92      0.96        13\n",
      "                               Rosebank       1.00      0.96      0.98        28\n",
      "                           Rowans Creek       1.00      0.92      0.96        12\n",
      "                          Royal Brackla       1.00      0.94      0.97        17\n",
      "                        Royal Lochnagar       1.00      0.92      0.96        24\n",
      "                       Russells Reserve       1.00      0.97      0.98        96\n",
      "                   Russells Reserve Rye       1.00      1.00      1.00        20\n",
      "                                Sazerac       0.98      0.92      0.95        48\n",
      "                                  Scapa       1.00      0.92      0.96        36\n",
      "                              Sheep Dip       1.00      1.00      1.00        12\n",
      "                          Smooth Ambler       1.00      0.98      0.99        42\n",
      "                               Speyburn       1.00      1.00      1.00        21\n",
      "                             Springbank       0.98      0.97      0.98       405\n",
      "                               Stagg Jr       0.96      0.95      0.95        76\n",
      "                           Still Waters       1.00      0.93      0.97        15\n",
      "                             Stranahans       1.00      0.96      0.98        23\n",
      "                             Strathisla       1.00      1.00      1.00        28\n",
      "                             Strathmill       1.00      0.91      0.95        22\n",
      "                             Stronachie       1.00      1.00      1.00        11\n",
      "                         Sullivans Cove       1.00      1.00      1.00        18\n",
      "                               Talisker       0.98      0.98      0.98       321\n",
      "                                 Tamdhu       1.00      0.93      0.96        56\n",
      "                             Tamnavulin       1.00      1.00      1.00        11\n",
      "                               Teachers       1.00      0.75      0.86        12\n",
      "                                Teeling       1.00      0.97      0.99        36\n",
      "                         Thomas H Handy       0.95      0.95      0.95        63\n",
      "                              Tobermory       0.99      0.98      0.98        91\n",
      "                                Tomatin       1.00      0.98      0.99       127\n",
      "                              Tomintoul       1.00      0.92      0.96        52\n",
      "                                Tormore       1.00      0.90      0.95        20\n",
      "                                Toronto       1.00      0.89      0.94         9\n",
      "                          Tullamore Dew       1.00      1.00      1.00        11\n",
      "                           Tullibardine       1.00      0.96      0.98        23\n",
      "                             Tyrconnell       1.00      0.89      0.94        19\n",
      "                             Van Winkle       0.90      0.92      0.91        50\n",
      "                        Very Old Barton       1.00      0.96      0.98        23\n",
      "                              WL Weller       0.96      0.88      0.92        82\n",
      "                               Westland       1.00      1.00      1.00        66\n",
      "                             WhistlePig       0.95      1.00      0.97        93\n",
      "                            Wild Turkey       0.98      0.98      0.98       240\n",
      "                    Wild Turkey 101 Rye       1.00      0.71      0.83         7\n",
      "                                Willett       1.00      0.97      0.98        61\n",
      "              Willett Family Estate Rye       0.94      0.95      0.95        87\n",
      "                       Woodford Reserve       0.99      1.00      0.99        66\n",
      "                          Writers Tears       1.00      1.00      1.00        11\n",
      "                               Yamazaki       1.00      0.90      0.95        48\n",
      "                            Yellow Spot       1.00      0.92      0.96        12\n",
      "\n",
      "                               accuracy                           0.97     20214\n",
      "                              macro avg       0.99      0.95      0.97     20214\n",
      "                           weighted avg       0.97      0.97      0.97     20214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_search_all_whiskies.predict(X_train)\n",
    "print(classification_report(y_train, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a02148e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Testing out prediction code\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m],[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfragrant, vanilla, melon, grass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrass, sherry, oaky\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalm, balanced\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      4\u001b[0m grid_search_all_whiskies\u001b[38;5;241m.\u001b[39mpredict(test)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing out prediction code\n",
    "test = np.array([['','',''],['fragrant, vanilla, melon, grass', 'grass, sherry, oaky', 'calm, balanced']])\n",
    "\n",
    "grid_search_all_whiskies.predict(test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5eb38cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=liblinear;, score=0.184 total time=  25.5s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=liblinear;, score=0.177 total time=  25.7s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=liblinear;, score=0.170 total time=  25.5s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=sag;, score=0.185 total time=  26.0s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=sag;, score=0.178 total time=  26.1s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=1, clf__penalty=l2, clf__solver=sag;, score=0.170 total time=  26.6s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=liblinear;, score=0.190 total time=  28.5s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=liblinear;, score=0.183 total time=  25.5s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=liblinear;, score=0.176 total time=  26.5s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=sag;, score=0.188 total time=  27.3s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=sag;, score=0.182 total time=  27.5s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=2, clf__penalty=l2, clf__solver=sag;, score=0.177 total time=  27.1s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=liblinear;, score=0.188 total time=  26.5s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=liblinear;, score=0.185 total time=  26.7s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=liblinear;, score=0.177 total time=  26.8s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=sag;, score=0.192 total time=  28.2s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=sag;, score=0.189 total time=  28.2s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=4, clf__penalty=l2, clf__solver=sag;, score=0.180 total time=  28.8s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=liblinear;, score=0.183 total time=  27.4s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=liblinear;, score=0.186 total time=  27.9s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=liblinear;, score=0.175 total time=  27.8s\n",
      "[CV 1/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=sag;, score=0.186 total time=  30.8s\n",
      "[CV 2/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=sag;, score=0.189 total time=  30.5s\n",
      "[CV 3/3] END clf=LogisticRegression(), clf__C=8, clf__penalty=l2, clf__solver=sag;, score=0.177 total time=  29.6s\n",
      "[CV 1/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.201 total time= 1.9min\n",
      "[CV 2/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.208 total time= 1.9min\n",
      "[CV 3/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.194 total time= 1.9min\n",
      "[CV 1/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.207 total time= 3.2min\n",
      "[CV 2/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.204 total time= 3.3min\n",
      "[CV 3/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=200, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.197 total time= 3.3min\n",
      "[CV 1/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.203 total time= 1.8min\n",
      "[CV 2/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.210 total time= 1.8min\n",
      "[CV 3/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=1000;, score=0.192 total time= 1.9min\n",
      "[CV 1/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.203 total time= 3.3min\n",
      "[CV 2/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.206 total time= 3.4min\n",
      "[CV 3/3] END clf=RandomForestClassifier(n_jobs=-1), clf__bootstrap=False, clf__max_depth=500, clf__max_features=sqrt, clf__n_estimators=2000;, score=0.197 total time= 3.4min\n",
      "[CV 1/3] END clf=SVC(), clf__C=2, clf__degree=1;, score=0.213 total time= 1.5min\n",
      "[CV 2/3] END clf=SVC(), clf__C=2, clf__degree=1;, score=0.209 total time= 1.5min\n",
      "[CV 3/3] END clf=SVC(), clf__C=2, clf__degree=1;, score=0.201 total time= 1.5min\n",
      "[CV 1/3] END clf=SVC(), clf__C=2, clf__degree=2;, score=0.213 total time= 1.5min\n",
      "[CV 2/3] END clf=SVC(), clf__C=2, clf__degree=2;, score=0.209 total time= 1.5min\n",
      "[CV 3/3] END clf=SVC(), clf__C=2, clf__degree=2;, score=0.201 total time= 1.5min\n",
      "[CV 1/3] END clf=SVC(), clf__C=3, clf__degree=1;, score=0.213 total time= 1.5min\n",
      "[CV 2/3] END clf=SVC(), clf__C=3, clf__degree=1;, score=0.208 total time= 1.5min\n",
      "[CV 3/3] END clf=SVC(), clf__C=3, clf__degree=1;, score=0.201 total time= 1.5min\n",
      "[CV 1/3] END clf=SVC(), clf__C=3, clf__degree=2;, score=0.213 total time= 1.5min\n",
      "[CV 2/3] END clf=SVC(), clf__C=3, clf__degree=2;, score=0.208 total time= 1.5min\n",
      "[CV 3/3] END clf=SVC(), clf__C=3, clf__degree=2;, score=0.201 total time= 1.5min\n",
      "[CV 1/3] END clf=SVC(), clf__C=4, clf__degree=1;, score=0.213 total time= 1.5min\n",
      "[CV 2/3] END clf=SVC(), clf__C=4, clf__degree=1;, score=0.208 total time= 1.5min\n",
      "[CV 3/3] END clf=SVC(), clf__C=4, clf__degree=1;, score=0.201 total time= 1.5min\n",
      "[CV 1/3] END clf=SVC(), clf__C=4, clf__degree=2;, score=0.213 total time= 1.5min\n",
      "[CV 2/3] END clf=SVC(), clf__C=4, clf__degree=2;, score=0.208 total time= 1.6min\n",
      "[CV 3/3] END clf=SVC(), clf__C=4, clf__degree=2;, score=0.201 total time= 1.6min\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Aberfeldy       0.00      0.00      0.00        22\n",
      "       Aberlour       0.21      0.24      0.22       121\n",
      "      Ailsa Bay       0.00      0.00      0.00         3\n",
      " Allt-A-Bhainne       0.00      0.00      0.00         3\n",
      "         AnCnoc       0.14      0.03      0.05        33\n",
      "         Ardbeg       0.20      0.42      0.27       196\n",
      "        Ardmore       0.60      0.08      0.14        39\n",
      "   Ardnamurchan       0.00      0.00      0.00         3\n",
      "          Arran       0.11      0.27      0.16       105\n",
      "   Auchentoshan       0.13      0.10      0.11        39\n",
      "      Auchroisk       1.00      0.12      0.22         8\n",
      "       Aultmore       0.33      0.08      0.12        13\n",
      "       Balblair       0.00      0.00      0.00        34\n",
      "      Ballechin       1.00      0.05      0.10        19\n",
      "      Balmenach       0.00      0.00      0.00        12\n",
      "       Balvenie       0.20      0.39      0.26       134\n",
      "      Ben Nevis       0.60      0.12      0.19        52\n",
      "       BenRiach       0.18      0.14      0.16        64\n",
      "      Benrinnes       0.75      0.12      0.20        51\n",
      "      Benromach       0.62      0.16      0.26        31\n",
      "       Bladnoch       0.75      0.20      0.32        15\n",
      "    Blair Athol       0.00      0.00      0.00        18\n",
      "        Bowmore       0.29      0.16      0.20       114\n",
      "        Braeval       0.00      0.00      0.00         6\n",
      "          Brora       1.00      0.08      0.14        13\n",
      "  Bruichladdich       0.16      0.34      0.22       177\n",
      "   Bunnahabhain       0.15      0.27      0.19       181\n",
      "    Campbeltown       0.00      0.00      0.00         6\n",
      "       Caol Ila       0.23      0.27      0.25       119\n",
      "    Caperdonich       0.00      0.00      0.00         4\n",
      "         Cardhu       0.00      0.00      0.00         8\n",
      "      Clynelish       0.36      0.42      0.39        69\n",
      "    Cragganmore       1.00      0.08      0.14        13\n",
      "  Craigellachie       0.50      0.03      0.06        31\n",
      "       Daftmill       0.00      0.00      0.00         3\n",
      "      Dailuaine       0.00      0.00      0.00        12\n",
      "     Dallas Dhu       0.00      0.00      0.00         4\n",
      "        Dalmore       0.50      0.10      0.16        41\n",
      "     Dalwhinnie       0.33      0.11      0.16        19\n",
      "       Deanston       1.00      0.05      0.10        20\n",
      "       Dufftown       0.00      0.00      0.00         5\n",
      "       Edradour       1.00      0.16      0.27        32\n",
      "    Fettercairn       0.40      0.15      0.22        13\n",
      "      Finlaggan       0.00      0.00      0.00        11\n",
      "     Glen Elgin       0.00      0.00      0.00        14\n",
      "   Glen Garioch       0.80      0.16      0.27        25\n",
      "     Glen Grant       0.12      0.03      0.05        32\n",
      "     Glen Keith       0.50      0.10      0.17        10\n",
      "     Glen Moray       0.50      0.09      0.15        34\n",
      "       Glen Ord       1.00      0.20      0.33         5\n",
      "    Glen Scotia       0.80      0.11      0.19        38\n",
      "      Glen Spey       0.00      0.00      0.00         9\n",
      "   Glenallachie       0.67      0.13      0.22        15\n",
      "     Glenburgie       0.50      0.06      0.10        18\n",
      "      Glencadam       0.00      0.00      0.00        10\n",
      "    Glendronach       0.22      0.47      0.30       159\n",
      "     Glendullan       0.00      0.00      0.00         3\n",
      "    Glenfarclas       0.22      0.22      0.22       114\n",
      "    Glenfiddich       0.23      0.31      0.27        93\n",
      "  Glenglassaugh       0.00      0.00      0.00        10\n",
      "      Glengoyne       0.25      0.04      0.07        50\n",
      "    Glenkinchie       0.00      0.00      0.00         8\n",
      "      Glenlivet       0.17      0.30      0.22       115\n",
      "     Glenlossie       0.00      0.00      0.00         8\n",
      "   Glenmorangie       0.16      0.25      0.19       127\n",
      "     Glenrothes       0.75      0.07      0.13        41\n",
      "   Glentauchers       0.00      0.00      0.00        16\n",
      "     Glenturret       0.00      0.00      0.00        11\n",
      "      Hazelburn       0.50      0.05      0.08        22\n",
      "  Highland Park       0.24      0.36      0.29       133\n",
      "         Ileach       1.00      0.25      0.40         4\n",
      "       Imperial       0.00      0.00      0.00         8\n",
      "      Inchgower       0.00      0.00      0.00         3\n",
      "     Inchmurrin       0.00      0.00      0.00         5\n",
      "           Jura       0.50      0.05      0.09        42\n",
      "      Kilchoman       0.46      0.24      0.32        90\n",
      "      Kilkerran       1.00      0.24      0.39        29\n",
      "      Knockando       0.00      0.00      0.00         4\n",
      "      Lagavulin       0.45      0.21      0.28       116\n",
      "      Laphroaig       0.31      0.58      0.40       251\n",
      "         Ledaig       0.59      0.22      0.32        86\n",
      "       Linkwood       0.33      0.04      0.07        26\n",
      "        Lismore       0.00      0.00      0.00         4\n",
      "     Littlemill       0.00      0.00      0.00        12\n",
      "    Loch Lomond       0.00      0.00      0.00         7\n",
      "       Longmorn       0.00      0.00      0.00        15\n",
      "        Longrow       0.39      0.16      0.23        67\n",
      "       Macallan       0.16      0.23      0.19       139\n",
      "    Mannochmore       0.00      0.00      0.00         8\n",
      "     Miltonduff       0.00      0.00      0.00         3\n",
      "       Mortlach       0.50      0.17      0.25        42\n",
      "           Oban       0.00      0.00      0.00        34\n",
      "       Octomore       0.56      0.11      0.18        95\n",
      "   Old Pulteney       0.40      0.14      0.21        57\n",
      "     Pittyvaich       0.00      0.00      0.00         4\n",
      "    Port Askaig       0.00      0.00      0.00         8\n",
      " Port Charlotte       0.20      0.07      0.11        68\n",
      "     Port Ellen       0.00      0.00      0.00        11\n",
      "       Rosebank       0.00      0.00      0.00        14\n",
      "  Royal Brackla       0.00      0.00      0.00         6\n",
      "Royal Lochnagar       1.00      0.50      0.67         8\n",
      "          Scapa       0.00      0.00      0.00         8\n",
      "       Speyburn       0.00      0.00      0.00         8\n",
      "     Springbank       0.21      0.32      0.25       132\n",
      "     Strathisla       0.00      0.00      0.00         8\n",
      "     Strathmill       0.00      0.00      0.00        10\n",
      "     Stronachie       0.00      0.00      0.00         1\n",
      "       Talisker       0.29      0.27      0.28       105\n",
      "         Tamdhu       0.00      0.00      0.00        25\n",
      "     Tamnavulin       0.00      0.00      0.00         2\n",
      "      Tobermory       0.00      0.00      0.00        33\n",
      "        Tomatin       0.67      0.06      0.11        32\n",
      "      Tomintoul       1.00      0.06      0.11        17\n",
      "        Tormore       1.00      0.12      0.22         8\n",
      "   Tullibardine       0.00      0.00      0.00         4\n",
      "\n",
      "       accuracy                           0.23      4695\n",
      "      macro avg       0.28      0.10      0.12      4695\n",
      "   weighted avg       0.30      0.23      0.21      4695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Then do just the scotch single malts\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
    "    df_sms[['nose', 'palate', 'finish']], df_sms['brand'])\n",
    "X2_train, X2_test = X2_train.to_numpy(), X2_test.to_numpy()\n",
    "\n",
    "grid_search_single_malts = GridSearchCV(estimator = pipeline, cv=3, \n",
    "                    param_grid = param_grid, scoring='accuracy', verbose=3)\n",
    "\n",
    "grid_search_single_malts.fit(X2_train, y2_train)\n",
    "\n",
    "y2_pred = grid_search_single_malts.predict(X2_test)\n",
    "\n",
    "print(classification_report(y2_test, y2_pred, zero_division=0))\n",
    "\n",
    "# Save to pickle file\n",
    "with open('./models/malt_classifier.pkl', \"wb\") as f:\n",
    "    joblib.dump(grid_search_single_malts, f, compress='zlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85915522",
   "metadata": {},
   "source": [
    "In the end, I decided not to proceed with the 'Just single malts' version. It did slightly better, with 23% accuracy as opposed to 21%, but not enough to justify the exclusion of so many interesting whiskies that make the app a truly international affair.\n",
    "\n",
    "As for the score itself, I doubt anyone is going to be blown away by an accuracy of 21%, but informal feedback from people who know their whisky is fairly positive. Even when the guesses are wrong, they make sense. The model is producing a multi-dimensional matrix where the Euclidean distance does seem to correspond more or less to one's intuitive sense of which whiskies are similar to each other, which is a good sign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccafb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
